{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b796b67b",
   "metadata": {},
   "source": [
    "# Testing different Variations on the Variational Autoencoder\n",
    "\n",
    "Biggest further improvement needs to be weighting each genre taste by the prevalance of genre so that way genres with more ratings on them don't hold higher weight\n",
    "\n",
    "## Input rating matrix info \n",
    "Here is the collaborative filtering component where we use the matrix of ratings by the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979204e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leuch\\AppData\\Local\\Temp\\ipykernel_8104\\2626815215.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  raw_data = pd.read_csv(\"./data/Movielens100/u.data\", sep = None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [2., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 2., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "raw_data = pd.read_csv(\"./data/Movielens100/u.data\", sep = None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "raw_data = raw_data.loc[:, raw_data.columns != \"timestamp\"]\n",
    "#make indices start at 0\n",
    "raw_data[\"userId\"] -= 1\n",
    "raw_data[\"movieId\"] -= 1\n",
    "#make ratings center around 0\n",
    "raw_data[\"rating\"] -= 3\n",
    "\n",
    "# create (943, 1682) matrix of user ratings per movie\n",
    "user_ratings = pd.DataFrame(np.zeros((943,1682)))\n",
    "for i in raw_data.index:\n",
    "    user_ratings[raw_data[\"movieId\"][i]][raw_data[\"userId\"][i]] = raw_data[\"rating\"][i]\n",
    "user_ratings = user_ratings.to_numpy() \n",
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "801771d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cefbd",
   "metadata": {},
   "source": [
    "## Import the user information matrix I made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ad8be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.824422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.528577</td>\n",
       "      <td>0.163282</td>\n",
       "      <td>-0.643886</td>\n",
       "      <td>-0.442094</td>\n",
       "      <td>-1.133953</td>\n",
       "      <td>0.682175</td>\n",
       "      <td>-0.240302</td>\n",
       "      <td>-0.297957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528577</td>\n",
       "      <td>-0.499749</td>\n",
       "      <td>-0.384440</td>\n",
       "      <td>-0.586232</td>\n",
       "      <td>-0.470922</td>\n",
       "      <td>0.624520</td>\n",
       "      <td>0.682175</td>\n",
       "      <td>0.365074</td>\n",
       "      <td>-0.067338</td>\n",
       "      <td>-0.442094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.554043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>0.549569</td>\n",
       "      <td>-0.003917</td>\n",
       "      <td>-0.419032</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>1.241427</td>\n",
       "      <td>0.411197</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.142289</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.280661</td>\n",
       "      <td>1.933286</td>\n",
       "      <td>-0.142289</td>\n",
       "      <td>0.411197</td>\n",
       "      <td>-0.280661</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.906438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-1.036383</td>\n",
       "      <td>-0.238085</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-1.355702</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.238085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.717064</td>\n",
       "      <td>-1.036383</td>\n",
       "      <td>-0.876723</td>\n",
       "      <td>-0.238085</td>\n",
       "      <td>-0.238085</td>\n",
       "      <td>-0.876723</td>\n",
       "      <td>-2.154000</td>\n",
       "      <td>-0.717064</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.824422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>1.621949</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>1.933286</td>\n",
       "      <td>1.621949</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.246068</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.999277</td>\n",
       "      <td>0.687941</td>\n",
       "      <td>0.999277</td>\n",
       "      <td>2.555958</td>\n",
       "      <td>0.376605</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.086278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.506365</td>\n",
       "      <td>-0.149094</td>\n",
       "      <td>-0.149094</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>-1.374024</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.149094</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608443</td>\n",
       "      <td>-0.455327</td>\n",
       "      <td>-1.220908</td>\n",
       "      <td>-0.353249</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-1.220908</td>\n",
       "      <td>0.310254</td>\n",
       "      <td>-0.608443</td>\n",
       "      <td>-0.404288</td>\n",
       "      <td>-0.608443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>-0.660390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>2.193939</td>\n",
       "      <td>0.166634</td>\n",
       "      <td>-0.412597</td>\n",
       "      <td>-0.412597</td>\n",
       "      <td>2.773170</td>\n",
       "      <td>0.021826</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.412597</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.412597</td>\n",
       "      <td>-0.412597</td>\n",
       "      <td>2.049132</td>\n",
       "      <td>0.745864</td>\n",
       "      <td>1.325094</td>\n",
       "      <td>0.311441</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>-0.168294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>0.284045</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.304969</td>\n",
       "      <td>-0.052535</td>\n",
       "      <td>1.293784</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.304969</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.473259</td>\n",
       "      <td>-0.220824</td>\n",
       "      <td>0.536480</td>\n",
       "      <td>-0.473259</td>\n",
       "      <td>0.031610</td>\n",
       "      <td>-0.220824</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>-1.152486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>1.933286</td>\n",
       "      <td>1.310613</td>\n",
       "      <td>0.999277</td>\n",
       "      <td>0.376605</td>\n",
       "      <td>1.933286</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.246068</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>1.621949</td>\n",
       "      <td>1.621949</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>-0.557404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>1.143963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>1.301320</td>\n",
       "      <td>1.208383</td>\n",
       "      <td>0.093149</td>\n",
       "      <td>1.022511</td>\n",
       "      <td>1.673064</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371532</td>\n",
       "      <td>-0.371532</td>\n",
       "      <td>-0.371532</td>\n",
       "      <td>0.093149</td>\n",
       "      <td>0.279022</td>\n",
       "      <td>1.673064</td>\n",
       "      <td>0.093149</td>\n",
       "      <td>1.301320</td>\n",
       "      <td>1.022511</td>\n",
       "      <td>-0.092723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>-0.988454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>0.999277</td>\n",
       "      <td>-0.157115</td>\n",
       "      <td>-0.512928</td>\n",
       "      <td>-0.646357</td>\n",
       "      <td>-0.646357</td>\n",
       "      <td>0.421081</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.601881</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>-0.157115</td>\n",
       "      <td>-0.423974</td>\n",
       "      <td>-0.468451</td>\n",
       "      <td>-0.290545</td>\n",
       "      <td>-0.557404</td>\n",
       "      <td>0.732417</td>\n",
       "      <td>-0.201591</td>\n",
       "      <td>-0.735311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1         2         3         4         5         6  \\\n",
       "0   -0.824422  1.0 -0.528577  0.163282 -0.643886 -0.442094 -1.133953   \n",
       "1    1.554043  0.0 -0.557404  0.549569 -0.003917 -0.419032 -0.557404   \n",
       "2   -0.906438  1.0 -0.557404 -1.036383 -0.238085 -0.557404 -0.557404   \n",
       "3   -0.824422  1.0 -0.557404  1.621949  0.065268 -0.557404 -0.557404   \n",
       "4   -0.086278  0.0 -0.506365 -0.149094 -0.149094  0.004022 -1.374024   \n",
       "..        ...  ...       ...       ...       ...       ...       ...   \n",
       "938 -0.660390  0.0 -0.557404  2.193939  0.166634 -0.412597 -0.412597   \n",
       "939 -0.168294  1.0 -0.557404  0.284045 -0.557404 -0.304969 -0.052535   \n",
       "940 -1.152486  1.0 -0.557404  1.933286  1.310613  0.999277  0.376605   \n",
       "941  1.143963  0.0 -0.557404  1.301320  1.208383  0.093149  1.022511   \n",
       "942 -0.988454  1.0 -0.557404  0.999277 -0.157115 -0.512928 -0.646357   \n",
       "\n",
       "            7         8         9  ...        11        12        13  \\\n",
       "0    0.682175 -0.240302 -0.297957  ... -0.528577 -0.499749 -0.384440   \n",
       "1    1.241427  0.411197 -0.557404  ... -0.557404 -0.142289 -0.557404   \n",
       "2   -1.355702 -0.557404 -0.238085  ... -0.557404 -0.717064 -1.036383   \n",
       "3    1.933286  1.621949  0.065268  ... -0.557404 -0.557404 -0.246068   \n",
       "4   -0.557404 -0.149094 -0.557404  ... -0.608443 -0.455327 -1.220908   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "938  2.773170  0.021826 -0.557404  ... -0.412597 -0.557404 -0.557404   \n",
       "939  1.293784  0.199900 -0.557404  ... -0.557404 -0.304969 -0.557404   \n",
       "940  1.933286 -0.557404 -0.557404  ... -0.557404 -0.557404 -0.557404   \n",
       "941  1.673064 -0.557404 -0.557404  ... -0.371532 -0.371532 -0.371532   \n",
       "942 -0.646357  0.421081 -0.557404  ... -0.601881 -0.557404 -0.157115   \n",
       "\n",
       "           14        15        16        17        18        19        20  \n",
       "0   -0.586232 -0.470922  0.624520  0.682175  0.365074 -0.067338 -0.442094  \n",
       "1   -0.557404 -0.280661  1.933286 -0.142289  0.411197 -0.280661 -0.557404  \n",
       "2   -0.876723 -0.238085 -0.238085 -0.876723 -2.154000 -0.717064 -0.557404  \n",
       "3    0.065268  0.999277  0.687941  0.999277  2.555958  0.376605 -0.557404  \n",
       "4   -0.353249 -0.557404 -1.220908  0.310254 -0.608443 -0.404288 -0.608443  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "938 -0.412597 -0.412597  2.049132  0.745864  1.325094  0.311441 -0.557404  \n",
       "939 -0.473259 -0.220824  0.536480 -0.473259  0.031610 -0.220824 -0.557404  \n",
       "940 -0.246068  0.065268  0.065268  1.621949  1.621949  0.065268 -0.557404  \n",
       "941  0.093149  0.279022  1.673064  0.093149  1.301320  1.022511 -0.092723  \n",
       "942 -0.423974 -0.468451 -0.290545 -0.557404  0.732417 -0.201591 -0.735311  \n",
       "\n",
       "[943 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info = pd.read_csv(\"./data/user_info.csv\", index_col = 0)\n",
    "user_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a471c42",
   "metadata": {},
   "source": [
    "## Create Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54377024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is the final labels as is\n",
    "labels = user_ratings\n",
    "\n",
    "#The features will be 80% of the users ratings concatenated with the user_info\n",
    "\n",
    "# create a mask of 0 and 1 values where half are 0 and 0.8 (Default) are 1. \n",
    "#The ratio of masked values is something that can and should be optimized. \n",
    "mask_magnitude = 1.3\n",
    "random_mask = np.clip((np.random.randn(1682) + mask_magnitude).round(), a_max = 1, a_min = 0)\n",
    "\n",
    "user_ratings *= random_mask\n",
    "features = np.concatenate((user_info, user_ratings), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931fe0c",
   "metadata": {},
   "source": [
    "## Make sure to shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f19ae",
   "metadata": {},
   "source": [
    "## Begin model and tuning of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eb094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn \n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "device = \"cuda\"\n",
    "\n",
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # input_noise The variance of the noise \n",
    "        \n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "#noise layers for regularization\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return x + torch.autograd.Variable(torch.randn(x.size()).cuda() * self.stddev)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# This is a standard VAE but using MSE as the reconstruction term. \n",
    "\n",
    "#Actually just rename this out and make loss function a parameter\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, recon_loss_fcn = \"MSE\", residual_user_info = False, dropout_rate = 0.0, nonLinearity = \"LeakyRelu\", fixed_variance = False, deterministicEval = False, noiseLayerStd = 0.2, hidden_size =1024, latent_size = 512):\n",
    "        #fixed variance is false if regular or equal to the value of the parameter\n",
    "        super().__init__()\n",
    "        \n",
    "        if nonLinearity == \"LeakyRelu\":\n",
    "            nonLin = nn.LeakyReLU\n",
    "        elif nonLinearity == \"Relu\":\n",
    "            nonLin = nn.ReLU\n",
    "        elif nonLinearity == \"Tanh\":\n",
    "            nonLin = nn.Tanh\n",
    "        elif nonLinearity == \"Sigmoid\":\n",
    "            nonLin = nn.Sigmoid\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            #Encoder\n",
    "            nn.Linear(1703, hidden_size),\n",
    "            nonLin(),\n",
    "            GaussianNoise(noiseLayerStd),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nonLin(),\n",
    "            GaussianNoise(noiseLayerStd),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            #Decoder\n",
    "            nn.Linear(latent_size,hidden_size),\n",
    "            nonLin(),\n",
    "            GaussianNoise(noiseLayerStd),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc_var = nn.Linear(hidden_size, latent_size)\n",
    "        final_layer_size = hidden_size\n",
    "        self.residual_user_info = residual_user_info\n",
    "        if self.residual_user_info == True:\n",
    "            final_layer_size += 22\n",
    "        self.final_layer = nn.Linear(final_layer_size,1682)\n",
    "        self.final_activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        #for evaluation purposes\n",
    "        self.test_mse = 1000\n",
    "        self.mse_loss_fcn = nn.MSELoss()\n",
    "        self.recon_loss_fcn = recon_loss_fcn\n",
    "        \n",
    "        # for varieties of VAE\n",
    "        self.fixed_variance = fixed_variance\n",
    "        self.deterministicEval = deterministicEval\n",
    "        \n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing data under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=1)\n",
    "    \n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded) if self.fixed_variance == False else self.fc_var(x_encoded) / self.fc_var(x_encoded) * self.fixed_variance\n",
    "        \n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        if self.training == False and self.deterministicEval == True:\n",
    "            std = std * 0\n",
    "    \n",
    "        #perform the kernel trick to allow for backprop through sampling\n",
    "        epsilon = torch.distributions.Normal(0, 1).rsample()\n",
    "        z = mu + epsilon * std\n",
    "        # decoded\n",
    "        if self.residual_user_info == True:\n",
    "            final_layer_input = torch.concat((self.decoder(z), x[:, :22]), axis = 1)\n",
    "        else:\n",
    "            final_layer_input = self.decoder(z)\n",
    "        ratings = self.final_layer(final_layer_input) \n",
    "        ratings = self.final_activation(ratings) * 2\n",
    "        return ratings, z, mu, std\n",
    "    \n",
    "    def vae_loss(self, x_hat, x, z, mu, std):\n",
    "        # reconstruction loss\n",
    "        if self.recon_loss_fcn == \"MSE\":\n",
    "            recon_loss = self.mse_loss_fcn(x_hat, x) * 10000\n",
    "        else: \n",
    "            recon_loss = -self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "        \n",
    "        # kl\n",
    "        if self.training == True and self.fixed_variance == False:\n",
    "            kl = self.kl_divergence(z, mu, std)\n",
    "        else:\n",
    "            kl = 0\n",
    "        \n",
    "        # elbo\n",
    "        elbo = (kl + recon_loss).mean()\n",
    "\n",
    "        return elbo\n",
    "\n",
    "def train(dataloader, model,  optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device).to(torch.float32), y.to(device).to(torch.float32)\n",
    "        \n",
    "        #compute prediction error\n",
    "        pred, z, mu, std = model(X)\n",
    "        loss = model.vae_loss(pred, y, z, mu, std)\n",
    "        mse_loss = mse_loss_fcn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0 and batch % 64 == 0:\n",
    "            loss , current = loss.item(), (batch+1) * len(X)\n",
    "            #print(\"Epoch : \" + str(epoch))\n",
    "            #print(f\"loss: {loss:>7f}\")\n",
    "            #print(f\"MSE loss: {mse_loss:>7f}\")\n",
    "            losses.append(loss)\n",
    "            mse_losses.append(mse_loss.item())\n",
    "\n",
    "def test(dataloader, model, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches= len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, test_mse_loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).to(torch.float32), y.to(device).to(torch.float32)\n",
    "            \n",
    "            \n",
    "            pred, z, mu, std = model(X)\n",
    "            test_loss += model.vae_loss(pred, y, z, mu, std).item()\n",
    "            test_mse_loss += mse_loss_fcn(pred, y)\n",
    "    test_loss /= num_batches\n",
    "    test_mse_loss /= num_batches\n",
    "    if epoch % 100 ==0:\n",
    "        test_losses.append(test_loss)\n",
    "        #Calculate RMSE on just the rated ones like the papers do \n",
    "        nonzero_indices = y.nonzero().split( 1, dim=1)\n",
    "        relevant_rmse = torch.sqrt(mse_loss_fcn(y[nonzero_indices], pred[nonzero_indices]))#torch.sqrt((y[nonzero_indices] - pred[nonzero_indices] ** 2).mean())\n",
    "        \n",
    "        #print(f\"Test Error: \\n Avg Loss : {test_loss:>8f} \")\n",
    "        #print(f\" Test MSE loss: {test_mse_loss:>7f}\")\n",
    "        #print(\"Relevant RMSE loss: \" + str(relevant_rmse.item()))\n",
    "        test_mse_losses.append(test_mse_loss.item())\n",
    "        model.test_mse = relevant_rmse.item()\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c3d997c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([4096, 1704]) from checkpoint, the shape in current model is torch.Size([2048, 1703]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([4096, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for fc_mu.weight: copying a param with shape torch.Size([512, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for fc_mu.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for fc_var.weight: copying a param with shape torch.Size([512, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for fc_var.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for final_layer.weight: copying a param with shape torch.Size([1682, 4118]) from checkpoint, the shape in current model is torch.Size([1682, 2070]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m shuffle(features, labels, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE(recon_loss_fcn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, residual_user_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, nonLinearity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, fixed_variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, deterministicEval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, noiseLayerStd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, latent_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m relevance_labels \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m843\u001b[39m:] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([4096, 1704]) from checkpoint, the shape in current model is torch.Size([2048, 1703]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([4096, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for fc_mu.weight: copying a param with shape torch.Size([512, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for fc_mu.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for fc_var.weight: copying a param with shape torch.Size([512, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for fc_var.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for final_layer.weight: copying a param with shape torch.Size([1682, 4118]) from checkpoint, the shape in current model is torch.Size([1682, 2070])."
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(features, labels, random_state=1)\n",
    "model = VAE(recon_loss_fcn = \"MSE\", residual_user_info = True, nonLinearity = \"Tanh\", dropout_rate = 0.1, fixed_variance = 0.5, deterministicEval = True, noiseLayerStd = 0.1, hidden_size = 2048, latent_size = 2048 ).to(device)\n",
    "model.load_state_dict(torch.load(\"./models/best.pth\"))\n",
    "model.eval()\n",
    "relevance_labels = y[843:] > 0.5\n",
    "#To evaluate for comparison to other papers we mask by setting 20% of labels to false \n",
    "test_inputs = X[843:]\n",
    "test_dataset = MovielensDataset(X[843:],y[843:])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    test_preds = model(torch.tensor(test_inputs).to(device).to(torch.float32))[0]\n",
    "    test_pred_labels = test_preds > .4\n",
    "    nonzero_indices = torch.tensor(test_inputs).to(device).nonzero().split( 1, dim=1)\n",
    "    test(test_dataloader, model, 100 )\n",
    "\n",
    "    #print(torch.sqrt(((torch.tensor(y[843:]).to(device)[nonzero_indices]- test_preds[nonzero_indices]) ** 2))/len(nonzero_indices[0]))\n",
    "    \n",
    "    print(precision_recall_fscore_support(relevance_labels.flatten(), test_pred_labels.cpu().flatten(), average=\"binary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf99dfa",
   "metadata": {},
   "source": [
    "## The actual search \n",
    "The following contains all of the logic you need to do to restart everything over so long as you've already loaded the ones above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cec19ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 11 using hyperparameters : ['MSE', 0.0001, False, 64, 1024, 2048, 0.0, 0.5, 0, 0.5, 'Adagrad', 'LeakyRelu', True]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(iteration) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m using hyperparameters : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(hyperparameters))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     test(test_dataloader, model, t )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[25], line 174\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer, epoch)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m#Backpropagation\u001b[39;00m\n\u001b[0;32m    173\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 174\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "epochs = 2500\n",
    "\n",
    "\n",
    "results = pd.read_csv(\"./data/model_results.csv\", header= 0, index_col = 0)\n",
    "\n",
    "all_time_lowest_mse = 0.5516\n",
    "iteration = 11\n",
    "while(True):\n",
    "    #New set of hyperparams\n",
    "    X, y = shuffle(features, labels, random_state=1)\n",
    "\n",
    "    lowest_test_mse_score = 1000\n",
    "    losses = []\n",
    "    mse_losses = []    \n",
    "    test_losses = []\n",
    "    test_mse_losses = []\n",
    "\n",
    "\n",
    "    #Search params\n",
    "    learning_rate = np.random.choice([5e-5, 1e-4])\n",
    "    deterministicEval = np.random.choice([True, False])\n",
    "    batch_size = int(np.random.choice([32, 64, 128]))\n",
    "    hidden_size = int(np.random.choice([1024, 2048, 4096]))\n",
    "    latent_size = int(np.random.choice([512, 1024, 2048]))\n",
    "    fixed_variance = np.random.choice([False, 0.1, 0.3, 0.5])\n",
    "    dropout_rate = float(np.random.choice([0.0, 0.1, 0.3, 0.5]))\n",
    "    l2_weight = float(np.random.choice([0.0, 1e-5, 1e-4, 1e-3]))\n",
    "    noiseLayerStd = float(np.random.choice([0.0, 0.1, 0.3, 0.5]))\n",
    "    optimizer_fcn = np.random.choice([\"Adam\", \"Adagrad\", \"RMSProp\"])\n",
    "    nonLinearity = np.random.choice([\"LeakyRelu\", \"Relu\", \"Tanh\", \"Sigmoid\"])\n",
    "    residual_user_info = np.random.choice([True, False])\n",
    "    recon_loss_fcn = np.random.choice([\"MSE\", \"Likelihood\"])\n",
    "\n",
    "    train_dataset = MovielensDataset(X[:843], y[:843])\n",
    "    test_dataset = MovielensDataset(X[843:],y[843:])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    model = VAE(recon_loss_fcn = recon_loss_fcn, residual_user_info = residual_user_info, nonLinearity = nonLinearity, dropout_rate = dropout_rate, fixed_variance = False, deterministicEval = deterministicEval, noiseLayerStd = noiseLayerStd, hidden_size = hidden_size, latent_size = latent_size ).to(device)\n",
    "\n",
    "    if optimizer_fcn == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr =  learning_rate, weight_decay = l2_weight)\n",
    "    elif optimizer_fcn == \"Adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr =  learning_rate)\n",
    "        l2_weight = 0\n",
    "    elif optimizer_fcn == \"RMSProp\":\n",
    "        learning_rate = np.random.choice([5e-6, 1e-5])\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "        l2_weight = 0\n",
    "\n",
    "    mse_loss_fcn = nn.MSELoss()\n",
    "\n",
    "    hyperparameters = [recon_loss_fcn, learning_rate, deterministicEval,  batch_size, hidden_size, latent_size, fixed_variance, dropout_rate, l2_weight, noiseLayerStd, optimizer_fcn, nonLinearity, residual_user_info]\n",
    "    print(\"iteration : \" + str(iteration) + \" using hyperparameters : \" + str(hyperparameters))\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train(train_dataloader, model,  optimizer, t)\n",
    "        test(test_dataloader, model, t )\n",
    "        if t % 100 == 0:\n",
    "            if model.test_mse < lowest_test_mse_score:\n",
    "                lowest_test_mse_score = model.test_mse\n",
    "                \n",
    "                \n",
    "                if model.test_mse < all_time_lowest_mse:\n",
    "                    torch.save(model.state_dict(), \"./models/best.pth\")\n",
    "                    print(\"NEW RECORD : \" + str(model.test_mse))\n",
    "                    print(\"New best using hyper parameters : \" + str(hyperparameters)  )\n",
    "                    all_time_lowest_mse = model.test_mse\n",
    "    hyperparameters.append(lowest_test_mse_score)\n",
    "    results = pd.concat((results, pd.DataFrame([ hyperparameters], columns= results.columns)))\n",
    "    results.to_csv(\"./data/model_results.csv\", index=True)\n",
    "    iteration+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37caa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(losses)) * 100, losses)\n",
    "plt.plot(np.arange(len(test_losses)) * 100, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91332c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(mse_losses)) * 100, np.sqrt(np.array(mse_losses)))\n",
    "plt.plot(np.arange(len(test_mse_losses)) * 100, np.sqrt(np.array(test_mse_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7e865",
   "metadata": {},
   "source": [
    "# Calculate Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49287493",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe48a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m test_pred_labels \u001b[38;5;241m=\u001b[39m test_preds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.4\u001b[39m\n\u001b[0;32m     13\u001b[0m nonzero_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_inputs)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msplit( \u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnonzero_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#print(torch.sqrt(((torch.tensor(y[843:]).to(device)[nonzero_indices]- test_preds[nonzero_indices]) ** 2))/len(nonzero_indices[0]))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(precision_recall_fscore_support(relevance_labels\u001b[38;5;241m.\u001b[39mflatten(), test_pred_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mflatten(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:338\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    336\u001b[0m                                  tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor_str.py:481\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 481\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor_str.py:447\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    445\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    446\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    450\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor_str.py:270\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor_str.py:103\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(features, labels, random_state=1)\n",
    "model = VAE(recon_loss_fcn = \"MSE\", residual_user_info = True, nonLinearity = \"Tanh\", dropout_rate = 0.1, fixed_variance = 0.5, deterministicEval = True, noiseLayerStd = 0.1, hidden_size = 2048, latent_size = 2048 ).to(device)\n",
    "model.load_state_dict(torch.load(\"./models/best.pth\"))\n",
    "model.eval()\n",
    "relevance_labels = y[843:] > 0.5\n",
    "#To evaluate for comparison to other papers we mask by setting 20% of labels to false \n",
    "test_inputs = X[843:]\n",
    "with torch.no_grad():\n",
    "    test_preds = model(torch.tensor(test_inputs).to(device).to(torch.float32))[0]\n",
    "    test_pred_labels = test_preds > .4\n",
    "    nonzero_indices = torch.tensor(test_inputs).to(device).nonzero().split( 1, dim=1)\n",
    "    print(test_preds)\n",
    "\n",
    "    #print(torch.sqrt(((torch.tensor(y[843:]).to(device)[nonzero_indices]- test_preds[nonzero_indices]) ** 2))/len(nonzero_indices[0]))\n",
    "\n",
    "    print(precision_recall_fscore_support(relevance_labels.flatten(), test_pred_labels.cpu().flatten(), average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd91cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7013190252626872, 0.6547693592151952, 0.677245250431779, None)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(precision_recall_fscore_support(relevance_labels\u001b[38;5;241m.\u001b[39mflatten(), test_preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mflatten(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m nonzero_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_inputs)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msplit( \u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msqrt(mse_loss_fcn(torch\u001b[38;5;241m.\u001b[39mtensor(y[\u001b[38;5;241m843\u001b[39m:])\u001b[38;5;241m.\u001b[39mto(device)[nonzero_indices], model(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m843\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))[\u001b[38;5;241m0\u001b[39m][nonzero_indices])))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = model(torch.tensor(test_inputs).to(device).to(torch.float32))[0] > .45\n",
    "    print(precision_recall_fscore_support(relevance_labels.flatten(), test_preds.cpu().flatten(), average=\"binary\"))\n",
    "    nonzero_indices = torch.tensor(test_inputs).to(device).nonzero().split( 1, dim=1)\n",
    "    print(torch.sqrt(mse_loss_fcn(torch.tensor(y[843:]).to(device)[nonzero_indices], model(torch.tensor(X[843:]).to(device).to(torch.float32))[0][nonzero_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdf260",
   "metadata": {},
   "source": [
    "# Calculate rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "549cf4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_indices = torch.tensor(test_inputs).to(\"cuda\").nonzero().split( 1, dim=1)\n",
    "torch.sqrt(((torch.tensor(test_inputs).to(\"cuda\")[nonzero_indices] - model(torch.tensor(test_inputs).to(\"cuda\").to(torch.float32))[0][nonzero_indices]) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bdefd",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The result is that I was, in fact, correct and it creates a noticeable improvement in the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "22542451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0000e+00, 1.1000e+01, 2.3000e+01, 1.0932e+04, 6.1663e+04,\n",
       "        7.1087e+04, 2.2324e+04, 7.7400e+02, 1.3480e+03, 3.8000e+01]),\n",
       " array([-2.1, -1.9, -1.1, -0.9, -0.1,  0. ,  0.1,  0.9,  1.1,  1.9,  2.1]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAs0ElEQVR4nO3dcVCU94H/8c8WZIsUnoLIbndCEu7KMFJMLiUZRJtqTgUzIM31JtojtxMvBk0xcnvCmNj8Udu5QqJG0zsmnvEyITWm2z880kxVCplrSDlFDVemwWguvZiAlRVT1wUps0vJ/v7IL89lwRoXY5Bv3q+ZZ8Z9ns+zz/fZHWc/891nHxzRaDQqAAAAA31hqgcAAABwrVB0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGSpzqAUylDz74QGfOnFFqaqocDsdUDwcAAFyBaDSqoaEheTwefeELl5+z+VwXnTNnzig7O3uqhwEAACahr69PN9xww2Uzn+uik5qaKunDFyotLW2KRwMAAK7E4OCgsrOz7c/xy/lcF52Pvq5KS0uj6AAAMM1cyWUnXIwMAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYKzEqR4AAFwrNz+6/4qz7z5edg1HAmCqxDWjc/PNN8vhcExY1q1bJ0mKRqPavHmzPB6PkpOTtWjRIh0/fjzmOcLhsNavX6/MzEylpKSooqJCp0+fjskEg0F5vV5ZliXLsuT1enXhwoWYTG9vr5YvX66UlBRlZmaqpqZGkUhkEi8BAAAwVVxF59ixY+rv77eXtrY2SdK9994rSdqyZYu2b9+uxsZGHTt2TG63W0uXLtXQ0JD9HD6fT83NzfL7/ero6NDFixdVXl6usbExO1NZWanu7m61tLSopaVF3d3d8nq99vaxsTGVlZVpeHhYHR0d8vv92rdvn2pra6/qxQAAAGZxRKPR6GR39vl8+sUvfqG3335bkuTxeOTz+fTII49I+nD2xuVy6YknntDatWsVCoU0e/Zs7dmzRytXrpQknTlzRtnZ2Tpw4IBKS0t14sQJ5efnq7OzU0VFRZKkzs5OFRcX6+TJk8rLy9PBgwdVXl6uvr4+eTweSZLf79eqVas0MDCgtLS0Kxr/4OCgLMtSKBS64n0ATB98dQWYKZ7P70lfjByJRPTCCy/ogQcekMPh0KlTpxQIBFRSUmJnnE6nFi5cqEOHDkmSurq6NDo6GpPxeDwqKCiwM4cPH5ZlWXbJkaR58+bJsqyYTEFBgV1yJKm0tFThcFhdXV2TPSUAAGCYSV+M/NJLL+nChQtatWqVJCkQCEiSXC5XTM7lcum9996zM0lJSUpPT5+Q+Wj/QCCgrKysCcfLysqKyYw/Tnp6upKSkuzMpYTDYYXDYfvx4ODglZwqAACYpiY9o/Pss8/q7rvvjplVkSSHwxHzOBqNTlg33vjMpfKTyYzX0NBgX+BsWZays7MvOy4AADC9TarovPfee3rllVf04IMP2uvcbrckTZhRGRgYsGdf3G63IpGIgsHgZTNnz56dcMxz587FZMYfJxgManR0dMJMz8dt2rRJoVDIXvr6+q70lAEAwDQ0qaLz3HPPKSsrS2Vl/3fxXk5Ojtxut/1LLOnD63ja29s1f/58SVJhYaFmzJgRk+nv71dPT4+dKS4uVigU0tGjR+3MkSNHFAqFYjI9PT3q7++3M62trXI6nSosLPyz43Y6nUpLS4tZAACAueK+RueDDz7Qc889p/vvv1+Jif+3u8PhkM/nU319vXJzc5Wbm6v6+nrNnDlTlZWVkiTLsrR69WrV1tZq1qxZysjIUF1dnebOnaslS5ZIkubMmaNly5apqqpKu3btkiStWbNG5eXlysvLkySVlJQoPz9fXq9XW7du1fnz51VXV6eqqirKCwAAsMVddF555RX19vbqgQcemLBt48aNGhkZUXV1tYLBoIqKitTa2qrU1FQ7s2PHDiUmJmrFihUaGRnR4sWL1dTUpISEBDuzd+9e1dTU2L/OqqioUGNjo709ISFB+/fvV3V1tRYsWKDk5GRVVlZq27Zt8Z4OAAAw2FXdR2e64z46gNm4jw5gps/kPjoAAADXO4oOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABgrcaoHAADxuPnR/VM9BADTCDM6AADAWBQdAABgLIoOAAAwVtxF5/e//73+/u//XrNmzdLMmTP1V3/1V+rq6rK3R6NRbd68WR6PR8nJyVq0aJGOHz8e8xzhcFjr169XZmamUlJSVFFRodOnT8dkgsGgvF6vLMuSZVnyer26cOFCTKa3t1fLly9XSkqKMjMzVVNTo0gkEu8pAQAAQ8VVdILBoBYsWKAZM2bo4MGDevPNN/Xkk0/qy1/+sp3ZsmWLtm/frsbGRh07dkxut1tLly7V0NCQnfH5fGpubpbf71dHR4cuXryo8vJyjY2N2ZnKykp1d3erpaVFLS0t6u7ultfrtbePjY2prKxMw8PD6ujokN/v1759+1RbW3sVLwcAADCJIxqNRq80/Oijj+q//uu/9Otf//qS26PRqDwej3w+nx555BFJH87euFwuPfHEE1q7dq1CoZBmz56tPXv2aOXKlZKkM2fOKDs7WwcOHFBpaalOnDih/Px8dXZ2qqioSJLU2dmp4uJinTx5Unl5eTp48KDKy8vV19cnj8cjSfL7/Vq1apUGBgaUlpb2ieczODgoy7IUCoWuKA9g6l2rX129+3jZNXleAJ++eD6/45rRefnll3X77bfr3nvvVVZWlm677Tbt3r3b3n7q1CkFAgGVlJTY65xOpxYuXKhDhw5Jkrq6ujQ6OhqT8Xg8KigosDOHDx+WZVl2yZGkefPmybKsmExBQYFdciSptLRU4XA45qu0jwuHwxocHIxZAACAueIqOu+884527typ3Nxc/fKXv9RDDz2kmpoa/eQnP5EkBQIBSZLL5YrZz+Vy2dsCgYCSkpKUnp5+2UxWVtaE42dlZcVkxh8nPT1dSUlJdma8hoYG+5ofy7KUnZ0dz+kDAIBpJq6i88EHH+jrX/+66uvrddttt2nt2rWqqqrSzp07Y3IOhyPmcTQanbBuvPGZS+Unk/m4TZs2KRQK2UtfX99lxwQAAKa3uIrOV77yFeXn58esmzNnjnp7eyVJbrdbkibMqAwMDNizL263W5FIRMFg8LKZs2fPTjj+uXPnYjLjjxMMBjU6OjphpucjTqdTaWlpMQsAADBXXEVnwYIFeuutt2LW/c///I9uuukmSVJOTo7cbrfa2trs7ZFIRO3t7Zo/f74kqbCwUDNmzIjJ9Pf3q6enx84UFxcrFArp6NGjdubIkSMKhUIxmZ6eHvX399uZ1tZWOZ1OFRYWxnNaAADAUHH9rat/+qd/0vz581VfX68VK1bo6NGjeuaZZ/TMM89I+vCrJJ/Pp/r6euXm5io3N1f19fWaOXOmKisrJUmWZWn16tWqra3VrFmzlJGRobq6Os2dO1dLliyR9OEs0bJly1RVVaVdu3ZJktasWaPy8nLl5eVJkkpKSpSfny+v16utW7fq/PnzqqurU1VVFTM1AABAUpxF54477lBzc7M2bdqkH/7wh8rJydFTTz2l++67z85s3LhRIyMjqq6uVjAYVFFRkVpbW5WammpnduzYocTERK1YsUIjIyNavHixmpqalJCQYGf27t2rmpoa+9dZFRUVamxstLcnJCRo//79qq6u1oIFC5ScnKzKykpt27Zt0i8GAAAwS1z30TEN99EBph/uowPgmt1HBwAAYDqh6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYK66is3nzZjkcjpjF7Xbb26PRqDZv3iyPx6Pk5GQtWrRIx48fj3mOcDis9evXKzMzUykpKaqoqNDp06djMsFgUF6vV5ZlybIseb1eXbhwISbT29ur5cuXKyUlRZmZmaqpqVEkEonz9AEAgMnintH52te+pv7+fnt544037G1btmzR9u3b1djYqGPHjsntdmvp0qUaGhqyMz6fT83NzfL7/ero6NDFixdVXl6usbExO1NZWanu7m61tLSopaVF3d3d8nq99vaxsTGVlZVpeHhYHR0d8vv92rdvn2prayf7OgAAAAMlxr1DYmLMLM5HotGonnrqKT322GP69re/LUl6/vnn5XK59OKLL2rt2rUKhUJ69tlntWfPHi1ZskSS9MILLyg7O1uvvPKKSktLdeLECbW0tKizs1NFRUWSpN27d6u4uFhvvfWW8vLy1NraqjfffFN9fX3yeDySpCeffFKrVq3Sj370I6WlpU36BQEAAOaIe0bn7bfflsfjUU5Ojr7zne/onXfekSSdOnVKgUBAJSUldtbpdGrhwoU6dOiQJKmrq0ujo6MxGY/Ho4KCAjtz+PBhWZZllxxJmjdvnizLiskUFBTYJUeSSktLFQ6H1dXV9WfHHg6HNTg4GLMAAABzxVV0ioqK9JOf/ES//OUvtXv3bgUCAc2fP19/+MMfFAgEJEkulytmH5fLZW8LBAJKSkpSenr6ZTNZWVkTjp2VlRWTGX+c9PR0JSUl2ZlLaWhosK/7sSxL2dnZ8Zw+AACYZuIqOnfffbf+9m//VnPnztWSJUu0f/9+SR9+RfURh8MRs080Gp2wbrzxmUvlJ5MZb9OmTQqFQvbS19d32XEBAIDp7ap+Xp6SkqK5c+fq7bfftq/bGT+jMjAwYM++uN1uRSIRBYPBy2bOnj074Vjnzp2LyYw/TjAY1Ojo6ISZno9zOp1KS0uLWQAAgLmuquiEw2GdOHFCX/nKV5STkyO32622tjZ7eyQSUXt7u+bPny9JKiws1IwZM2Iy/f396unpsTPFxcUKhUI6evSonTly5IhCoVBMpqenR/39/XamtbVVTqdThYWFV3NKAADAIHH96qqurk7Lly/XjTfeqIGBAf3zP/+zBgcHdf/998vhcMjn86m+vl65ubnKzc1VfX29Zs6cqcrKSkmSZVlavXq1amtrNWvWLGVkZKiurs7+KkyS5syZo2XLlqmqqkq7du2SJK1Zs0bl5eXKy8uTJJWUlCg/P19er1dbt27V+fPnVVdXp6qqKmZpAACALa6ic/r0af3d3/2d3n//fc2ePVvz5s1TZ2enbrrpJknSxo0bNTIyourqagWDQRUVFam1tVWpqan2c+zYsUOJiYlasWKFRkZGtHjxYjU1NSkhIcHO7N27VzU1NfavsyoqKtTY2GhvT0hI0P79+1VdXa0FCxYoOTlZlZWV2rZt21W9GAAAwCyOaDQanepBTJXBwUFZlqVQKMRMEDBN3Pzo/mvyvO8+XnZNnhfApy+ez2/+1hUAADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsa6q6DQ0NMjhcMjn89nrotGoNm/eLI/Ho+TkZC1atEjHjx+P2S8cDmv9+vXKzMxUSkqKKioqdPr06ZhMMBiU1+uVZVmyLEter1cXLlyIyfT29mr58uVKSUlRZmamampqFIlEruaUAACAQSZddI4dO6ZnnnlGt9xyS8z6LVu2aPv27WpsbNSxY8fkdru1dOlSDQ0N2Rmfz6fm5mb5/X51dHTo4sWLKi8v19jYmJ2prKxUd3e3Wlpa1NLSou7ubnm9Xnv72NiYysrKNDw8rI6ODvn9fu3bt0+1tbWTPSUAAGCYSRWdixcv6r777tPu3buVnp5ur49Go3rqqaf02GOP6dvf/rYKCgr0/PPP649//KNefPFFSVIoFNKzzz6rJ598UkuWLNFtt92mF154QW+88YZeeeUVSdKJEyfU0tKif//3f1dxcbGKi4u1e/du/eIXv9Bbb70lSWptbdWbb76pF154QbfddpuWLFmiJ598Urt379bg4ODVvi4AAMAAkyo669atU1lZmZYsWRKz/tSpUwoEAiopKbHXOZ1OLVy4UIcOHZIkdXV1aXR0NCbj8XhUUFBgZw4fPizLslRUVGRn5s2bJ8uyYjIFBQXyeDx2prS0VOFwWF1dXZccdzgc1uDgYMwCAADMlRjvDn6/X//93/+tY8eOTdgWCAQkSS6XK2a9y+XSe++9Z2eSkpJiZoI+yny0fyAQUFZW1oTnz8rKismMP056erqSkpLszHgNDQ36wQ9+cCWnCQAADBDXjE5fX5/+8R//US+88IK++MUv/tmcw+GIeRyNRiesG2985lL5yWQ+btOmTQqFQvbS19d32TEBAIDpLa6i09XVpYGBARUWFioxMVGJiYlqb2/Xv/zLvygxMdGeYRk/ozIwMGBvc7vdikQiCgaDl82cPXt2wvHPnTsXkxl/nGAwqNHR0QkzPR9xOp1KS0uLWQAAgLniKjqLFy/WG2+8oe7ubnu5/fbbdd9996m7u1t/8Rd/Ibfbrba2NnufSCSi9vZ2zZ8/X5JUWFioGTNmxGT6+/vV09NjZ4qLixUKhXT06FE7c+TIEYVCoZhMT0+P+vv77Uxra6ucTqcKCwsn8VIAAADTxHWNTmpqqgoKCmLWpaSkaNasWfZ6n8+n+vp65ebmKjc3V/X19Zo5c6YqKyslSZZlafXq1aqtrdWsWbOUkZGhuro6zZ071764ec6cOVq2bJmqqqq0a9cuSdKaNWtUXl6uvLw8SVJJSYny8/Pl9Xq1detWnT9/XnV1daqqqmKmBgAASJrExcifZOPGjRoZGVF1dbWCwaCKiorU2tqq1NRUO7Njxw4lJiZqxYoVGhkZ0eLFi9XU1KSEhAQ7s3fvXtXU1Ni/zqqoqFBjY6O9PSEhQfv371d1dbUWLFig5ORkVVZWatu2bZ/2KQEAgGnKEY1Go1M9iKkyODgoy7IUCoWYBQKmiZsf3X9Nnvfdx8uuyfMC+PTF8/nN37oCAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGCuuorNz507dcsstSktLU1pamoqLi3Xw4EF7ezQa1ebNm+XxeJScnKxFixbp+PHjMc8RDoe1fv16ZWZmKiUlRRUVFTp9+nRMJhgMyuv1yrIsWZYlr9erCxcuxGR6e3u1fPlypaSkKDMzUzU1NYpEInGePgAAMFlcReeGG27Q448/rtdff12vv/66/vqv/1rf+ta37DKzZcsWbd++XY2NjTp27JjcbreWLl2qoaEh+zl8Pp+am5vl9/vV0dGhixcvqry8XGNjY3amsrJS3d3damlpUUtLi7q7u+X1eu3tY2NjKisr0/DwsDo6OuT3+7Vv3z7V1tZe7esBAAAM4ohGo9GreYKMjAxt3bpVDzzwgDwej3w+nx555BFJH87euFwuPfHEE1q7dq1CoZBmz56tPXv2aOXKlZKkM2fOKDs7WwcOHFBpaalOnDih/Px8dXZ2qqioSJLU2dmp4uJinTx5Unl5eTp48KDKy8vV19cnj8cjSfL7/Vq1apUGBgaUlpZ2RWMfHByUZVkKhUJXvA+AqXXzo/uvyfO++3jZNXleAJ++eD6/J32NztjYmPx+v4aHh1VcXKxTp04pEAiopKTEzjidTi1cuFCHDh2SJHV1dWl0dDQm4/F4VFBQYGcOHz4sy7LskiNJ8+bNk2VZMZmCggK75EhSaWmpwuGwurq6/uyYw+GwBgcHYxYAAGCuuIvOG2+8oS996UtyOp166KGH1NzcrPz8fAUCAUmSy+WKybtcLntbIBBQUlKS0tPTL5vJysqacNysrKyYzPjjpKenKykpyc5cSkNDg33dj2VZys7OjvPsAQDAdBJ30cnLy1N3d7c6Ozv13e9+V/fff7/efPNNe7vD4YjJR6PRCevGG5+5VH4ymfE2bdqkUChkL319fZcdFwAAmN7iLjpJSUn66le/qttvv10NDQ269dZb9eMf/1hut1uSJsyoDAwM2LMvbrdbkUhEwWDwspmzZ89OOO65c+diMuOPEwwGNTo6OmGm5+OcTqf9i7GPFgAAYK6rvo9ONBpVOBxWTk6O3G632tra7G2RSETt7e2aP3++JKmwsFAzZsyIyfT396unp8fOFBcXKxQK6ejRo3bmyJEjCoVCMZmenh719/fbmdbWVjmdThUWFl7tKQEAAEMkxhP+3ve+p7vvvlvZ2dkaGhqS3+/Xq6++qpaWFjkcDvl8PtXX1ys3N1e5ubmqr6/XzJkzVVlZKUmyLEurV69WbW2tZs2apYyMDNXV1Wnu3LlasmSJJGnOnDlatmyZqqqqtGvXLknSmjVrVF5erry8PElSSUmJ8vPz5fV6tXXrVp0/f151dXWqqqpilgYAANjiKjpnz56V1+tVf3+/LMvSLbfcopaWFi1dulSStHHjRo2MjKi6ulrBYFBFRUVqbW1Vamqq/Rw7duxQYmKiVqxYoZGRES1evFhNTU1KSEiwM3v37lVNTY3966yKigo1Njba2xMSErR//35VV1drwYIFSk5OVmVlpbZt23ZVLwYAADDLVd9HZzrjPjrA9MN9dAB8JvfRAQAAuN5RdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjJU41QMAgOvBzY/un+ohGOndx8umegj4nGNGBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWHEVnYaGBt1xxx1KTU1VVlaW7rnnHr311lsxmWg0qs2bN8vj8Sg5OVmLFi3S8ePHYzLhcFjr169XZmamUlJSVFFRodOnT8dkgsGgvF6vLMuSZVnyer26cOFCTKa3t1fLly9XSkqKMjMzVVNTo0gkEs8pAQAAg8VVdNrb27Vu3Tp1dnaqra1Nf/rTn1RSUqLh4WE7s2XLFm3fvl2NjY06duyY3G63li5dqqGhITvj8/nU3Nwsv9+vjo4OXbx4UeXl5RobG7MzlZWV6u7uVktLi1paWtTd3S2v12tvHxsbU1lZmYaHh9XR0SG/3699+/aptrb2al4PAABgEEc0Go1Odudz584pKytL7e3t+uY3v6loNCqPxyOfz6dHHnlE0oezNy6XS0888YTWrl2rUCik2bNna8+ePVq5cqUk6cyZM8rOztaBAwdUWlqqEydOKD8/X52dnSoqKpIkdXZ2qri4WCdPnlReXp4OHjyo8vJy9fX1yePxSJL8fr9WrVqlgYEBpaWlfeL4BwcHZVmWQqHQFeUBTD1u7De9cMNAXAvxfH5f1TU6oVBIkpSRkSFJOnXqlAKBgEpKSuyM0+nUwoULdejQIUlSV1eXRkdHYzIej0cFBQV25vDhw7Isyy45kjRv3jxZlhWTKSgosEuOJJWWliocDqurq+uS4w2HwxocHIxZAACAuSZddKLRqDZs2KBvfOMbKigokCQFAgFJksvlism6XC57WyAQUFJSktLT0y+bycrKmnDMrKysmMz446SnpyspKcnOjNfQ0GBf82NZlrKzs+M9bQAAMI1Muug8/PDD+u1vf6uf/vSnE7Y5HI6Yx9FodMK68cZnLpWfTObjNm3apFAoZC99fX2XHRMAAJjeJlV01q9fr5dfflm/+tWvdMMNN9jr3W63JE2YURkYGLBnX9xutyKRiILB4GUzZ8+enXDcc+fOxWTGHycYDGp0dHTCTM9HnE6n0tLSYhYAAGCuuIpONBrVww8/rP/4j//Qf/7nfyonJydme05Ojtxut9ra2ux1kUhE7e3tmj9/viSpsLBQM2bMiMn09/erp6fHzhQXFysUCuno0aN25siRIwqFQjGZnp4e9ff325nW1lY5nU4VFhbGc1oAAMBQifGE161bpxdffFE///nPlZqaas+oWJal5ORkORwO+Xw+1dfXKzc3V7m5uaqvr9fMmTNVWVlpZ1evXq3a2lrNmjVLGRkZqqur09y5c7VkyRJJ0pw5c7Rs2TJVVVVp165dkqQ1a9aovLxceXl5kqSSkhLl5+fL6/Vq69atOn/+vOrq6lRVVcVMDQAAkBRn0dm5c6ckadGiRTHrn3vuOa1atUqStHHjRo2MjKi6ulrBYFBFRUVqbW1Vamqqnd+xY4cSExO1YsUKjYyMaPHixWpqalJCQoKd2bt3r2pqauxfZ1VUVKixsdHenpCQoP3796u6uloLFixQcnKyKisrtW3btrheAAAAYK6ruo/OdMd9dIDph/voTC/cRwfXwmd2Hx0AAIDrGUUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLESp3oAgKlufnT/VA8BAD734p7Ree2117R8+XJ5PB45HA699NJLMduj0ag2b94sj8ej5ORkLVq0SMePH4/JhMNhrV+/XpmZmUpJSVFFRYVOnz4dkwkGg/J6vbIsS5Zlyev16sKFCzGZ3t5eLV++XCkpKcrMzFRNTY0ikUi8pwQAAAwVd9EZHh7WrbfeqsbGxktu37Jli7Zv367GxkYdO3ZMbrdbS5cu1dDQkJ3x+Xxqbm6W3+9XR0eHLl68qPLyco2NjdmZyspKdXd3q6WlRS0tLeru7pbX67W3j42NqaysTMPDw+ro6JDf79e+fftUW1sb7ykBAABDOaLRaHTSOzscam5u1j333CPpw9kcj8cjn8+nRx55RNKHszcul0tPPPGE1q5dq1AopNmzZ2vPnj1auXKlJOnMmTPKzs7WgQMHVFpaqhMnTig/P1+dnZ0qKiqSJHV2dqq4uFgnT55UXl6eDh48qPLycvX19cnj8UiS/H6/Vq1apYGBAaWlpX3i+AcHB2VZlkKh0BXlgXjw1RUgvft42VQPAQaK5/P7U70Y+dSpUwoEAiopKbHXOZ1OLVy4UIcOHZIkdXV1aXR0NCbj8XhUUFBgZw4fPizLsuySI0nz5s2TZVkxmYKCArvkSFJpaanC4bC6urouOb5wOKzBwcGYBQAAmOtTLTqBQECS5HK5Yta7XC57WyAQUFJSktLT0y+bycrKmvD8WVlZMZnxx0lPT1dSUpKdGa+hocG+5seyLGVnZ0/iLAEAwHRxTX5e7nA4Yh5Ho9EJ68Ybn7lUfjKZj9u0aZNCoZC99PX1XXZMAABgevtUi47b7ZakCTMqAwMD9uyL2+1WJBJRMBi8bObs2bMTnv/cuXMxmfHHCQaDGh0dnTDT8xGn06m0tLSYBQAAmOtTLTo5OTlyu91qa2uz10UiEbW3t2v+/PmSpMLCQs2YMSMm09/fr56eHjtTXFysUCiko0eP2pkjR44oFArFZHp6etTf329nWltb5XQ6VVhY+GmeFgAAmKbivmHgxYsX9bvf/c5+fOrUKXV3dysjI0M33nijfD6f6uvrlZubq9zcXNXX12vmzJmqrKyUJFmWpdWrV6u2tlazZs1SRkaG6urqNHfuXC1ZskSSNGfOHC1btkxVVVXatWuXJGnNmjUqLy9XXl6eJKmkpET5+fnyer3aunWrzp8/r7q6OlVVVTFTAwAAJE2i6Lz++uu666677McbNmyQJN1///1qamrSxo0bNTIyourqagWDQRUVFam1tVWpqan2Pjt27FBiYqJWrFihkZERLV68WE1NTUpISLAze/fuVU1Njf3rrIqKiph79yQkJGj//v2qrq7WggULlJycrMrKSm3bti3+VwEAABjpqu6jM91xHx1cS9xHB+A+Org2puw+OgAAANcTig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjEXRAQAAxqLoAAAAY1F0AACAsSg6AADAWBQdAABgLIoOAAAwFkUHAAAYi6IDAACMRdEBAADGougAAABjUXQAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMBZFBwAAGIuiAwAAjJU41QMAACAeNz+6f6qHYKR3Hy+b6iFcE8zoAAAAY037GZ2nn35aW7duVX9/v772ta/pqaee0p133jnVwwIAiNkXTL1pPaPzs5/9TD6fT4899ph+85vf6M4779Tdd9+t3t7eqR4aAAC4DkzrorN9+3atXr1aDz74oObMmaOnnnpK2dnZ2rlz51QPDQAAXAem7VdXkUhEXV1devTRR2PWl5SU6NChQ5fcJxwOKxwO249DoZAkaXBw8NoNFJ9bH4T/ONVDAIArNp0+Cz8aazQa/cTstC0677//vsbGxuRyuWLWu1wuBQKBS+7T0NCgH/zgBxPWZ2dnX5MxAgAwXVhPTfUI4jc0NCTLsi6bmbZF5yMOhyPmcTQanbDuI5s2bdKGDRvsxx988IHOnz+vWbNm/dl9rrXBwUFlZ2err69PaWlpUzIGXDner+mD92r64L2aXq6H9ysajWpoaEgej+cTs9O26GRmZiohIWHC7M3AwMCEWZ6POJ1OOZ3OmHVf/vKXr9UQ45KWlsZ/8GmE92v64L2aPnivppepfr8+aSbnI9P2YuSkpCQVFhaqra0tZn1bW5vmz58/RaMCAADXk2k7oyNJGzZskNfr1e23367i4mI988wz6u3t1UMPPTTVQwMAANeBaV10Vq5cqT/84Q/64Q9/qP7+fhUUFOjAgQO66aabpnpoV8zpdOr73//+hK/UcH3i/Zo+eK+mD96r6WW6vV+O6JX8NgsAAGAamrbX6AAAAHwSig4AADAWRQcAABiLogMAAIxF0bmOvPvuu1q9erVycnKUnJysv/zLv9T3v/99RSKRqR4aLuFHP/qR5s+fr5kzZ143N57E/3n66aeVk5OjL37xiyosLNSvf/3rqR4SLuG1117T8uXL5fF45HA49NJLL031kHAJDQ0NuuOOO5SamqqsrCzdc889euutt6Z6WFeEonMdOXnypD744APt2rVLx48f144dO/Rv//Zv+t73vjfVQ8MlRCIR3Xvvvfrud7871UPBOD/72c/k8/n02GOP6Te/+Y3uvPNO3X333ert7Z3qoWGc4eFh3XrrrWpsbJzqoeAy2tvbtW7dOnV2dqqtrU1/+tOfVFJSouHh4ake2ifi5+XXua1bt2rnzp165513pnoo+DOamprk8/l04cKFqR4K/r+ioiJ9/etf186dO+11c+bM0T333KOGhoYpHBkux+FwqLm5Wffcc89UDwWf4Ny5c8rKylJ7e7u++c1vTvVwLosZnetcKBRSRkbGVA8DmDYikYi6urpUUlISs76kpESHDh2aolEBZgmFQpI0LT6fKDrXsf/93//Vv/7rv/InLYA4vP/++xobG5vwx31dLteEPwIMIH7RaFQbNmzQN77xDRUUFEz1cD4RReczsHnzZjkcjssur7/+esw+Z86c0bJly3TvvffqwQcfnKKRf/5M5r3C9cnhcMQ8jkajE9YBiN/DDz+s3/72t/rpT3861UO5ItP6b11NFw8//LC+853vXDZz88032/8+c+aM7rrrLvsPleKzE+97hetPZmamEhISJszeDAwMTJjlARCf9evX6+WXX9Zrr72mG264YaqHc0UoOp+BzMxMZWZmXlH297//ve666y4VFhbqueee0xe+wKTbZyme9wrXp6SkJBUWFqqtrU1/8zd/Y69va2vTt771rSkcGTB9RaNRrV+/Xs3NzXr11VeVk5Mz1UO6YhSd68iZM2e0aNEi3Xjjjdq2bZvOnTtnb3O73VM4MlxKb2+vzp8/r97eXo2Njam7u1uS9NWvflVf+tKXpnZwn3MbNmyQ1+vV7bffbs+M9vb2cr3bdejixYv63e9+Zz8+deqUuru7lZGRoRtvvHEKR4aPW7dunV588UX9/Oc/V2pqqj1jalmWkpOTp3h0l8fPy68jTU1N+od/+IdLbuNtuv6sWrVKzz///IT1v/rVr7Ro0aLPfkCI8fTTT2vLli3q7+9XQUGBduzYcd3/DPbz6NVXX9Vdd901Yf3999+vpqamz35AuKQ/d33bc889p1WrVn22g4kTRQcAABiLC0AAAICxKDoAAMBYFB0AAGAsig4AADAWRQcAABiLogMAAIxF0QEAAMai6AAAAGNRdAAAgLEoOgAAwFgUHQAAYCyKDgAAMNb/AyDpjRj8TuvhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(model(torch.tensor(test_inputs).to(\"cuda\").to(torch.float32))[0].cpu().detach().numpy().flatten(), bins = [-2.1, -1.9, -1.1, -0.9, -.1, 0, .1, .9, 1.1, 1.9, 2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7aa2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_inputs.flatten(), bins = [-2.1, -1.9, -1.1, -0.9, -.1, 0, .1, .9, 1.1, 1.9, 2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c6c95734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04012485136741974"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings[843:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8fff5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recon Loss</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>DetEval</th>\n",
       "      <th>batch size</th>\n",
       "      <th>hidden size</th>\n",
       "      <th>latent size</th>\n",
       "      <th>fixed variance</th>\n",
       "      <th>dropout rate</th>\n",
       "      <th>L2 weight</th>\n",
       "      <th>noise layer std</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>nonlinearity</th>\n",
       "      <th>residual user info</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.548719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recon Loss  Learning Rate  DetEval  batch size  hidden size  latent size  \\\n",
       "200  Likelihood         0.0001     True        32.0       4096.0       2048.0   \n",
       "\n",
       "     fixed variance  dropout rate  L2 weight noise layer std optimizer  \\\n",
       "200             0.0           0.0        0.0             0.0      Adam   \n",
       "\n",
       "    nonlinearity residual user info      RMSE  \n",
       "200         Tanh               True  0.548719  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"./data/model_results.csv\", header= 0, index_col = 0)\n",
    "results[results.RMSE == results.RMSE.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae91bc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recon Loss</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>DetEval</th>\n",
       "      <th>batch size</th>\n",
       "      <th>hidden size</th>\n",
       "      <th>latent size</th>\n",
       "      <th>fixed variance</th>\n",
       "      <th>dropout rate</th>\n",
       "      <th>L2 weight</th>\n",
       "      <th>noise layer std</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>nonlinearity</th>\n",
       "      <th>residual user info</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.548719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>False</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.595479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>False</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RMSProp</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>False</td>\n",
       "      <td>0.599642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>False</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RMSProp</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.614660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>False</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.645789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>True</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>Relu</td>\n",
       "      <td>True</td>\n",
       "      <td>1.991779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>True</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>LeakyRelu</td>\n",
       "      <td>True</td>\n",
       "      <td>2.009543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>False</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>Relu</td>\n",
       "      <td>True</td>\n",
       "      <td>2.023314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>True</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>Relu</td>\n",
       "      <td>True</td>\n",
       "      <td>2.036075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Likelihood</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>True</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>LeakyRelu</td>\n",
       "      <td>True</td>\n",
       "      <td>2.058407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recon Loss  Learning Rate  DetEval  batch size  hidden size  latent size  \\\n",
       "200  Likelihood        0.00010     True        32.0       4096.0       2048.0   \n",
       "0           MSE        0.00010    False        64.0       4096.0        512.0   \n",
       "124  Likelihood        0.00001    False        32.0       4096.0       2048.0   \n",
       "0           MSE        0.00001    False        32.0       4096.0        512.0   \n",
       "0           MSE        0.00010    False       128.0       4096.0        512.0   \n",
       "..          ...            ...      ...         ...          ...          ...   \n",
       "0           MSE        0.00005     True        64.0       1024.0        512.0   \n",
       "0    Likelihood        0.00005     True       128.0       2048.0       1024.0   \n",
       "138  Likelihood        0.00005    False       128.0       2048.0        512.0   \n",
       "67          MSE        0.00005     True        64.0       2048.0       2048.0   \n",
       "41   Likelihood        0.00005     True       128.0       4096.0        512.0   \n",
       "\n",
       "     fixed variance  dropout rate  L2 weight noise layer std optimizer  \\\n",
       "200             0.0           0.0      0.000             0.0      Adam   \n",
       "0               0.3           0.1      0.001             0.0      Adam   \n",
       "124             0.0           0.0      0.000             0.0   RMSProp   \n",
       "0               0.1           0.0      0.000             0.1   RMSProp   \n",
       "0               0.3           0.1      0.000             0.1      Adam   \n",
       "..              ...           ...        ...             ...       ...   \n",
       "0               0.0           0.5      0.000             0.3   Adagrad   \n",
       "0               0.5           0.5      0.000             0.1   Adagrad   \n",
       "138             0.1           0.5      0.000             0.0   Adagrad   \n",
       "67              0.0           0.3      0.000             0.0   Adagrad   \n",
       "41              0.0           0.5      0.000             0.3   Adagrad   \n",
       "\n",
       "    nonlinearity residual user info      RMSE  \n",
       "200         Tanh               True  0.548719  \n",
       "0           Tanh               True  0.595479  \n",
       "124         Tanh              False  0.599642  \n",
       "0           Tanh               True  0.614660  \n",
       "0           Tanh               True  0.645789  \n",
       "..           ...                ...       ...  \n",
       "0           Relu               True  1.991779  \n",
       "0      LeakyRelu               True  2.009543  \n",
       "138         Relu               True  2.023314  \n",
       "67          Relu               True  2.036075  \n",
       "41     LeakyRelu               True  2.058407  \n",
       "\n",
       "[546 rows x 14 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e6ffd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 11 using hyperparameters : ['MSE', 0.0001, False, 64, 1024, 2048, 0.0, 0.5, 0.001, 0.5, 'Adam', 'LeakyRelu', True]\n",
      "NEW RECORD : 0.5755844116210938\n",
      "New best using hyper parameters : ['MSE', 0.0001, False, 64, 1024, 2048, 0.0, 0.5, 0.001, 0.5, 'Adam', 'LeakyRelu', True]\n",
      "NEW RECORD : 0.5585877895355225\n",
      "New best using hyper parameters : ['MSE', 0.0001, False, 64, 1024, 2048, 0.0, 0.5, 0.001, 0.5, 'Adam', 'LeakyRelu', True]\n",
      "NEW RECORD : 0.5474879145622253\n",
      "New best using hyper parameters : ['MSE', 0.0001, False, 64, 1024, 2048, 0.0, 0.5, 0.001, 0.5, 'Adam', 'LeakyRelu', True]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "\n",
    "model = VAE(recon_loss_fcn = \"MSE\", residual_user_info = True, nonLinearity = \"Tanh\", dropout_rate = 0.1, fixed_variance = 0.3, deterministicEval = True, noiseLayerStd = 0.0, hidden_size = 4096, latent_size = 512 ).to(device)\n",
    "optimizer_fcn = \"Adam\"\n",
    "learning_rate = 0.0001\n",
    "l2_weight = 0.001\n",
    "if optimizer_fcn == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr =  learning_rate, weight_decay = l2_weight)\n",
    "elif optimizer_fcn == \"Adagrad\":\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr =  learning_rate)\n",
    "    l2_weight = 0\n",
    "elif optimizer_fcn == \"RMSProp\":\n",
    "    learning_rate = np.random.choice([5e-6, 1e-5])\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "    l2_weight = 0\n",
    "\n",
    "mse_loss_fcn = nn.MSELoss()\n",
    "\n",
    "hyperparameters = [recon_loss_fcn, learning_rate, deterministicEval,  batch_size, hidden_size, latent_size, fixed_variance, dropout_rate, l2_weight, noiseLayerStd, optimizer_fcn, nonLinearity, residual_user_info]\n",
    "print(\"iteration : \" + str(iteration) + \" using hyperparameters : \" + str(hyperparameters))\n",
    "\n",
    "all_time_lowest_mse = 0.57954519\n",
    "\n",
    "for t in range(epochs):\n",
    "    train(train_dataloader, model,  optimizer, t)\n",
    "    test(test_dataloader, model, t )\n",
    "    if t % 100 == 0:\n",
    "        if model.test_mse < lowest_test_mse_score:\n",
    "            lowest_test_mse_score = model.test_mse\n",
    "\n",
    "\n",
    "            if model.test_mse < all_time_lowest_mse:\n",
    "                torch.save(model.state_dict(), \"./models/real_best.pth\")\n",
    "                print(\"NEW RECORD : \" + str(model.test_mse))\n",
    "                print(\"New best using hyper parameters : \" + str(hyperparameters)  )\n",
    "                all_time_lowest_mse = model.test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f191e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your agequit\n",
      "Enter your sex (M/F)quit\n",
      "How do you feel about movie name?\n",
      "quit\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m     respones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow do you feel about movie name?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mresponse\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "age = input(\"Enter your age \\n\")\n",
    "sex = input(\"Enter your sex (M/F) \\n\")\n",
    "ratings = {}\n",
    "while(True):\n",
    "    respones = input(\"How do you feel about movie name?\\n\")\n",
    "    if (response == \"quit\"):\n",
    "        break\n",
    "    else:\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753e6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
